{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HQ-SAM (from train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from segment_anything_training import sam_model_registry\n",
    "from segment_anything_training.modeling import TwoWayTransformer, MaskDecoder\n",
    "\n",
    "from utils.dataloader import get_im_gt_name_dict, create_dataloaders, RandomHFlip, Resize, LargeScaleJitter\n",
    "from utils.loss_mask import loss_masks\n",
    "import utils.misc as misc"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LayerNorm2d\n",
    "Z-Score Normalization = Standardization(표준화)\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True) # 분산 계산\n",
    "        x = (x - u) / torch.sqrt(s + self.eps) # Z-Score Normalization = Standardization(표준화)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP (Multilayer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers # mlp layer 수 지정\n",
    "        h = [hidden_dim] * (num_layers - 1) # hidden layer의 dimension 지정\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output # output에 sig 유무 지정\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x) # for문을 반복하며 layer 위치에 맞게 relu 적용\n",
    "        if self.sigmoid_output: # sigmoid_output이 true일 경우 sigmoid 적용\n",
    "            x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *(중요 부분) MaskDecoderHQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDecoderHQ(MaskDecoder):\n",
    "    def __init__(self, model_type):\n",
    "        super().__init__(transformer_dim=256,\n",
    "                        # TwoWayTransformer의 경우 \n",
    "                        transformer=TwoWayTransformer(\n",
    "                                depth=2,\n",
    "                                embedding_dim=256,\n",
    "                                mlp_dim=2048,\n",
    "                                num_heads=8,\n",
    "                            ),\n",
    "                        num_multimask_outputs=3,\n",
    "                        activation=nn.GELU,\n",
    "                        iou_head_depth= 3,\n",
    "                        iou_head_hidden_dim= 256,)\n",
    "        assert model_type in [\"vit_b\", \"vit_l\", \"vit_h\"]\n",
    "\n",
    "        # pretrain 된 vit weight 파일\n",
    "        checkpoint_dict = {\"vit_b\":\"pretrained_checkpoint/sam_vit_b_maskdecoder.pth\",\n",
    "                           \"vit_l\":\"pretrained_checkpoint/sam_vit_l_maskdecoder.pth\",\n",
    "                           'vit_h':\"pretrained_checkpoint/sam_vit_h_maskdecoder.pth\"}\n",
    "        checkpoint_path = checkpoint_dict[model_type]\n",
    "        self.load_state_dict(torch.load(checkpoint_path))\n",
    "        print(\"HQ Decoder init from SAM MaskDecoder\")\n",
    "        \n",
    "        # parameter 업데이트를 막음\n",
    "        for n,p in self.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        transformer_dim=256\n",
    "        vit_dim_dict = {\"vit_b\":768,\"vit_l\":1024,\"vit_h\":1280}\n",
    "        # 지정한 모델 타입에 따라 dimention 조절\n",
    "        vit_dim = vit_dim_dict[model_type]\n",
    "        \n",
    "        # HQ_token 생성 \n",
    "        self.hf_token = nn.Embedding(1, transformer_dim)\n",
    "        self.hf_mlp = MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n",
    "        self.num_mask_tokens = self.num_mask_tokens + 1\n",
    "        \n",
    "        # ViT로 출력된 feature map을 ConvTranspose2d를 사용하여 upscaling (channel을 줄이고 size를 늘림)\n",
    "        self.compress_vit_feat = nn.Sequential(\n",
    "                                        nn.ConvTranspose2d(vit_dim, transformer_dim, kernel_size=2, stride=2),\n",
    "                                        LayerNorm2d(transformer_dim),\n",
    "                                        nn.GELU(), \n",
    "                                        nn.ConvTranspose2d(transformer_dim, transformer_dim // 8, kernel_size=2, stride=2))\n",
    "        \n",
    "        self.embedding_encoder = nn.Sequential(\n",
    "                                        nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n",
    "                                        LayerNorm2d(transformer_dim // 4),\n",
    "                                        nn.GELU(),\n",
    "                                        nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n",
    "                                    )\n",
    "\n",
    "        self.embedding_maskfeature = nn.Sequential(\n",
    "                                        nn.Conv2d(transformer_dim // 8, transformer_dim // 4, 3, 1, 1), \n",
    "                                        LayerNorm2d(transformer_dim // 4),\n",
    "                                        nn.GELU(),\n",
    "                                        nn.Conv2d(transformer_dim // 4, transformer_dim // 8, 3, 1, 1))\n",
    "        \n",
    "    def forward(\n",
    "    self,\n",
    "    image_embeddings: torch.Tensor,\n",
    "    image_pe: torch.Tensor,\n",
    "    sparse_prompt_embeddings: torch.Tensor,\n",
    "    dense_prompt_embeddings: torch.Tensor,\n",
    "    multimask_output: bool,\n",
    "    hq_token_only: bool,\n",
    "    interm_embeddings: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict masks given image and prompt embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          image_embeddings (torch.Tensor): the embeddings from the ViT image encoder\n",
    "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
    "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
    "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
    "          multimask_output (bool): Whether to return multiple masks or a single\n",
    "            mask.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: batched predicted hq masks\n",
    "        \"\"\"\n",
    "        \n",
    "        #conv에 넣기 위해 (B, C, H, W) 형태로 변경\n",
    "        vit_features = interm_embeddings[0].permute(0, 3, 1, 2) # early-layer ViT feature, after 1st global attention block in ViT\n",
    "        # ViT Feat (Global-local Fusion)\n",
    "        hq_features = self.embedding_encoder(image_embeddings) + self.compress_vit_feat(vit_features)\n",
    "\n",
    "        # 각 배치별 Mask & iou 생성\n",
    "        batch_len = len(image_embeddings)\n",
    "        masks = []\n",
    "        iou_preds = []\n",
    "        for i_batch in range(batch_len):\n",
    "            mask, iou_pred = self.predict_masks(\n",
    "                image_embeddings=image_embeddings[i_batch].unsqueeze(0),\n",
    "                image_pe=image_pe[i_batch],\n",
    "                #sparse_prompt_embeddings: the embeddings of the points and boxes\n",
    "                sparse_prompt_embeddings=sparse_prompt_embeddings[i_batch],\n",
    "                dense_prompt_embeddings=dense_prompt_embeddings[i_batch],\n",
    "                hq_feature = hq_features[i_batch].unsqueeze(0)\n",
    "            )\n",
    "            masks.append(mask)\n",
    "            iou_preds.append(iou_pred)\n",
    "        masks = torch.cat(masks,0)\n",
    "        iou_preds = torch.cat(iou_preds,0)\n",
    "\n",
    "        # Select the correct mask or masks for output\n",
    "        if multimask_output:\n",
    "            # mask with highest score (slice를 이용하여 list 추출 범위 지정)\n",
    "            mask_slice = slice(1,self.num_mask_tokens-1)\n",
    "            iou_preds = iou_preds[:, mask_slice]\n",
    "            iou_preds, max_iou_idx = torch.max(iou_preds,dim=1)\n",
    "            iou_preds = iou_preds.unsqueeze(1)\n",
    "            masks_multi = masks[:, mask_slice, :, :]\n",
    "            masks_sam = masks_multi[torch.arange(masks_multi.size(0)),max_iou_idx].unsqueeze(1)\n",
    "        else:\n",
    "            # masks의 첫번째 값 추출\n",
    "            mask_slice = slice(0, 1)\n",
    "            masks_sam = masks[:,mask_slice]\n",
    "\n",
    "        masks_hq = masks[:,slice(self.num_mask_tokens-1, self.num_mask_tokens), :, :]\n",
    "        \n",
    "        if hq_token_only:\n",
    "            return masks_hq\n",
    "        else:\n",
    "            return masks_sam, masks_hq\n",
    "    \n",
    "    def predict_masks(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "        dense_prompt_embeddings: torch.Tensor,\n",
    "        hq_feature: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "          Arguments:\n",
    "          image_embeddings (torch.Tensor): the embeddings from the ViT image encoder\n",
    "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
    "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
    "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight, self.hf_token.weight], dim=0)\n",
    "        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
    "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
    "\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0) # torch.repeat_interleave(input, repeat, dim) 반복 횟수만큼 텐서 반복\n",
    "        src = src + dense_prompt_embeddings # image imbedding + embeddings of the mask inputs\n",
    "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        # Run the transformer\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "        iou_token_out = hs[:, 0, :]\n",
    "        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n",
    "\n",
    "        # Upscale mask embeddings and predict masks using the mask tokens\n",
    "        src = src.transpose(1, 2).view(b, c, h, w)\n",
    "\n",
    "        upscaled_embedding_sam = self.output_upscaling(src)\n",
    "        upscaled_embedding_ours = self.embedding_maskfeature(upscaled_embedding_sam) + hq_feature\n",
    "        \n",
    "        hyper_in_list: List[torch.Tensor] = []\n",
    "        for i in range(self.num_mask_tokens):\n",
    "            if i < 4:\n",
    "                hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
    "            else:\n",
    "                hyper_in_list.append(self.hf_mlp(mask_tokens_out[:, i, :]))\n",
    "\n",
    "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        b, c, h, w = upscaled_embedding_sam.shape\n",
    "\n",
    "        masks_sam = (hyper_in[:,:4] @ upscaled_embedding_sam.view(b, c, h * w)).view(b, -1, h, w)\n",
    "        masks_ours = (hyper_in[:,4:] @ upscaled_embedding_ours.view(b, c, h * w)).view(b, -1, h, w)\n",
    "        masks = torch.cat([masks_sam,masks_ours],dim=1)\n",
    "        \n",
    "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
    "\n",
    "        return masks, iou_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
